{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "data = load_iris()\n",
    "# x = data['data']\n",
    "# y = data['target']\n",
    "#"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#\n",
    "#\n",
    "# # x, y = Variable(torch.tensor(x,dtype=torch.float)), Variable(torch.tensor(y,dtype=torch.float))\n",
    "#\n",
    "# class NN(torch.nn.Module):\n",
    "#     def __init__(self,n_input=4, n_output=1):\n",
    "#         super(NN, self).__init__()\n",
    "#         self.fc = torch.nn.Linear(n_input,n_output)\n",
    "#         self.relu = torch.nn.ReLU() # instead of Heaviside step fn\n",
    "#     def forward(self, x):\n",
    "#         x = self.fc(x)\n",
    "#         # print(x)\n",
    "#         output = self.relu(x) # instead of Heaviside step fn\n",
    "#         return output\n",
    "#\n",
    "# net = NN(n_input=4, n_output=1)\n",
    "#"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "# class Trainer:\n",
    "#     loss_history = []\n",
    "#     x = None\n",
    "#     y = None\n",
    "#     model = None\n",
    "#\n",
    "#     def __init__(self,model,epoch,loss_func,optimizer):\n",
    "#         self.model = model # initialized model\n",
    "#         self.epoch = epoch\n",
    "#         self.loss_func = loss_func\n",
    "#         self.optimizer = torch.optim.SGD(model.parameters(), lr=0.2)\n",
    "#\n",
    "#     def fit(self,X,Y):\n",
    "#         x = Variable(torch.tensor(X,dtype=torch.float))\n",
    "#         y = Variable(torch.tensor(Y,dtype=torch.float))\n",
    "#         model = self.model\n",
    "#         self.loss_history = []\n",
    "#         for t in range(self.epoch):\n",
    "#\n",
    "#             prediction = model(x).squeeze()\n",
    "#             # print(prediction)\n",
    "#\n",
    "#             loss = self.loss_func(prediction, y)\n",
    "#             self.loss_history.append(loss)\n",
    "#             print(loss)\n",
    "#\n",
    "#             self.optimizer.zero_grad()\n",
    "#             print(f'Epoch {t}: train loss: {loss}')\n",
    "#             loss.backward()\n",
    "#             self.optimizer.step()\n",
    "#\n",
    "#\n",
    "#     def plot_loss_history(self):\n",
    "#         plt.plot(self.loss_history)\n",
    "#"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "# model = NN(n_input=4,n_output=1)\n",
    "#\n",
    "# t = Trainer(model=model,epoch=5,loss_func=torch.nn.MSELoss())\n",
    "#"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "# t.fit(X=data['data'],Y=data['target'])\n",
    "#"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "# t.loss_history"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 0.44461295008659363\n",
      "Epoch 1: train loss: 0.3108506500720978\n",
      "Epoch 2: train loss: 0.28323858976364136\n",
      "Epoch 3: train loss: 0.26372674107551575\n",
      "Epoch 4: train loss: 0.2461971789598465\n",
      "Epoch 5: train loss: 0.2301524579524994\n",
      "Epoch 6: train loss: 0.21544824540615082\n",
      "Epoch 7: train loss: 0.20197135210037231\n",
      "Epoch 8: train loss: 0.18961912393569946\n",
      "Epoch 9: train loss: 0.17829769849777222\n",
      "Epoch 10: train loss: 0.16792084276676178\n",
      "Epoch 11: train loss: 0.1584097295999527\n",
      "Epoch 12: train loss: 0.14969202876091003\n",
      "Epoch 13: train loss: 0.14170147478580475\n",
      "Epoch 14: train loss: 0.13437733054161072\n",
      "Epoch 15: train loss: 0.12766388058662415\n",
      "Epoch 16: train loss: 0.12151014059782028\n",
      "Epoch 17: train loss: 0.11586938053369522\n",
      "Epoch 18: train loss: 0.11069869995117188\n",
      "Epoch 19: train loss: 0.10595888644456863\n",
      "Epoch 20: train loss: 0.10161393135786057\n",
      "Epoch 21: train loss: 0.0976308286190033\n",
      "Epoch 22: train loss: 0.0939793810248375\n",
      "Epoch 23: train loss: 0.09063185006380081\n",
      "Epoch 24: train loss: 0.08756288886070251\n",
      "Epoch 25: train loss: 0.08474922925233841\n",
      "Epoch 26: train loss: 0.08216951042413712\n",
      "Epoch 27: train loss: 0.0798041969537735\n",
      "Epoch 28: train loss: 0.07763543725013733\n",
      "Epoch 29: train loss: 0.07564675062894821\n",
      "Epoch 30: train loss: 0.07382312417030334\n",
      "Epoch 31: train loss: 0.07215076684951782\n",
      "Epoch 32: train loss: 0.07061706483364105\n",
      "Epoch 33: train loss: 0.06921039521694183\n",
      "Epoch 34: train loss: 0.06792017817497253\n",
      "Epoch 35: train loss: 0.06673669070005417\n",
      "Epoch 36: train loss: 0.06565099209547043\n",
      "Epoch 37: train loss: 0.06465496122837067\n",
      "Epoch 38: train loss: 0.06374108791351318\n",
      "Epoch 39: train loss: 0.06290250271558762\n",
      "Epoch 40: train loss: 0.06213292479515076\n",
      "Epoch 41: train loss: 0.06142660230398178\n",
      "Epoch 42: train loss: 0.06077824905514717\n",
      "Epoch 43: train loss: 0.06018299236893654\n",
      "Epoch 44: train loss: 0.05963645875453949\n",
      "Epoch 45: train loss: 0.05913453921675682\n",
      "Epoch 46: train loss: 0.05867352336645126\n",
      "Epoch 47: train loss: 0.05824998766183853\n",
      "Epoch 48: train loss: 0.05786082521080971\n",
      "Epoch 49: train loss: 0.05750314146280289\n",
      "Epoch 50: train loss: 0.057174332439899445\n",
      "Epoch 51: train loss: 0.05687194690108299\n",
      "Epoch 52: train loss: 0.05659385025501251\n",
      "Epoch 53: train loss: 0.056337952613830566\n",
      "Epoch 54: train loss: 0.05610242858529091\n",
      "Epoch 55: train loss: 0.05588558316230774\n",
      "Epoch 56: train loss: 0.055685855448246\n",
      "Epoch 57: train loss: 0.05550181865692139\n",
      "Epoch 58: train loss: 0.05533213913440704\n",
      "Epoch 59: train loss: 0.05517565459012985\n",
      "Epoch 60: train loss: 0.05503125488758087\n",
      "Epoch 61: train loss: 0.05489793047308922\n",
      "Epoch 62: train loss: 0.05477477237582207\n",
      "Epoch 63: train loss: 0.05466090515255928\n",
      "Epoch 64: train loss: 0.05455557629466057\n",
      "Epoch 65: train loss: 0.054458070546388626\n",
      "Epoch 66: train loss: 0.05436773970723152\n",
      "Epoch 67: train loss: 0.054283980280160904\n",
      "Epoch 68: train loss: 0.0542062371969223\n",
      "Epoch 69: train loss: 0.054134052246809006\n",
      "Epoch 70: train loss: 0.05406691133975983\n",
      "Epoch 71: train loss: 0.054004427045583725\n",
      "Epoch 72: train loss: 0.053946200758218765\n",
      "Epoch 73: train loss: 0.05389189347624779\n",
      "Epoch 74: train loss: 0.05384116992354393\n",
      "Epoch 75: train loss: 0.05379371717572212\n",
      "Epoch 76: train loss: 0.05374930053949356\n",
      "Epoch 77: train loss: 0.05370763689279556\n",
      "Epoch 78: train loss: 0.053668517619371414\n",
      "Epoch 79: train loss: 0.05363171920180321\n",
      "Epoch 80: train loss: 0.05359707400202751\n",
      "Epoch 81: train loss: 0.05356438457965851\n",
      "Epoch 82: train loss: 0.05353348329663277\n",
      "Epoch 83: train loss: 0.053504232317209244\n",
      "Epoch 84: train loss: 0.05347650498151779\n",
      "Epoch 85: train loss: 0.053450170904397964\n",
      "Epoch 86: train loss: 0.05342510715126991\n",
      "Epoch 87: train loss: 0.053401220589876175\n",
      "Epoch 88: train loss: 0.05337841063737869\n",
      "Epoch 89: train loss: 0.053356610238552094\n",
      "Epoch 90: train loss: 0.053335703909397125\n",
      "Epoch 91: train loss: 0.053315628319978714\n",
      "Epoch 92: train loss: 0.053296323865652084\n",
      "Epoch 93: train loss: 0.053277742117643356\n",
      "Epoch 94: train loss: 0.053259797394275665\n",
      "Epoch 95: train loss: 0.05324244126677513\n",
      "Epoch 96: train loss: 0.05322565883398056\n",
      "Epoch 97: train loss: 0.053209371864795685\n",
      "Epoch 98: train loss: 0.053193554282188416\n",
      "Epoch 99: train loss: 0.053178176283836365\n",
      "Epoch 100: train loss: 0.053163185715675354\n",
      "Epoch 101: train loss: 0.053148556500673294\n",
      "Epoch 102: train loss: 0.05313427373766899\n",
      "Epoch 103: train loss: 0.05312030017375946\n",
      "Epoch 104: train loss: 0.05310662090778351\n",
      "Epoch 105: train loss: 0.05309320613741875\n",
      "Epoch 106: train loss: 0.05308002978563309\n",
      "Epoch 107: train loss: 0.053067099303007126\n",
      "Epoch 108: train loss: 0.053054362535476685\n",
      "Epoch 109: train loss: 0.05304183438420296\n",
      "Epoch 110: train loss: 0.05302949622273445\n",
      "Epoch 111: train loss: 0.05301731824874878\n",
      "Epoch 112: train loss: 0.053005293011665344\n",
      "Epoch 113: train loss: 0.05299341306090355\n",
      "Epoch 114: train loss: 0.0529816709458828\n",
      "Epoch 115: train loss: 0.05297005549073219\n",
      "Epoch 116: train loss: 0.05295855924487114\n",
      "Epoch 117: train loss: 0.052947185933589935\n",
      "Epoch 118: train loss: 0.05293590575456619\n",
      "Epoch 119: train loss: 0.05292472988367081\n",
      "Epoch 120: train loss: 0.05291365087032318\n",
      "Epoch 121: train loss: 0.05290265381336212\n",
      "Epoch 122: train loss: 0.05289173126220703\n",
      "Epoch 123: train loss: 0.05288088321685791\n",
      "Epoch 124: train loss: 0.05287011340260506\n",
      "Epoch 125: train loss: 0.052859410643577576\n",
      "Epoch 126: train loss: 0.052848782390356064\n",
      "Epoch 127: train loss: 0.05283821001648903\n",
      "Epoch 128: train loss: 0.052827704697847366\n",
      "Epoch 129: train loss: 0.052817244082689285\n",
      "Epoch 130: train loss: 0.05280684307217598\n",
      "Epoch 131: train loss: 0.052796486765146255\n",
      "Epoch 132: train loss: 0.05278618633747101\n",
      "Epoch 133: train loss: 0.05277593061327934\n",
      "Epoch 134: train loss: 0.052765727043151855\n",
      "Epoch 135: train loss: 0.05275556072592735\n",
      "Epoch 136: train loss: 0.05274542421102524\n",
      "Epoch 137: train loss: 0.0527353473007679\n",
      "Epoch 138: train loss: 0.05272531136870384\n",
      "Epoch 139: train loss: 0.05271530896425247\n",
      "Epoch 140: train loss: 0.05270534008741379\n",
      "Epoch 141: train loss: 0.05269540846347809\n",
      "Epoch 142: train loss: 0.05268552154302597\n",
      "Epoch 143: train loss: 0.052675653249025345\n",
      "Epoch 144: train loss: 0.0526658296585083\n",
      "Epoch 145: train loss: 0.05265603959560394\n",
      "Epoch 146: train loss: 0.052646271884441376\n",
      "Epoch 147: train loss: 0.05263654887676239\n",
      "Epoch 148: train loss: 0.0526268370449543\n",
      "Epoch 149: train loss: 0.05261717364192009\n",
      "Epoch 150: train loss: 0.05260754004120827\n",
      "Epoch 151: train loss: 0.052597928792238235\n",
      "Epoch 152: train loss: 0.05258835852146149\n",
      "Epoch 153: train loss: 0.05257880687713623\n",
      "Epoch 154: train loss: 0.05256928130984306\n",
      "Epoch 155: train loss: 0.05255979299545288\n",
      "Epoch 156: train loss: 0.052550315856933594\n",
      "Epoch 157: train loss: 0.05254087597131729\n",
      "Epoch 158: train loss: 0.052531469613313675\n",
      "Epoch 159: train loss: 0.05252208560705185\n",
      "Epoch 160: train loss: 0.05251272767782211\n",
      "Epoch 161: train loss: 0.052503395825624466\n",
      "Epoch 162: train loss: 0.052494097501039505\n",
      "Epoch 163: train loss: 0.05248481407761574\n",
      "Epoch 164: train loss: 0.05247556045651436\n",
      "Epoch 165: train loss: 0.052466318011283875\n",
      "Epoch 166: train loss: 0.05245712399482727\n",
      "Epoch 167: train loss: 0.05244794487953186\n",
      "Epoch 168: train loss: 0.05243879184126854\n",
      "Epoch 169: train loss: 0.05242966488003731\n",
      "Epoch 170: train loss: 0.05242056027054787\n",
      "Epoch 171: train loss: 0.05241149291396141\n",
      "Epoch 172: train loss: 0.052402425557374954\n",
      "Epoch 173: train loss: 0.05239339917898178\n",
      "Epoch 174: train loss: 0.0523843877017498\n",
      "Epoch 175: train loss: 0.05237540602684021\n",
      "Epoch 176: train loss: 0.05236644297838211\n",
      "Epoch 177: train loss: 0.052357517182826996\n",
      "Epoch 178: train loss: 0.05234859511256218\n",
      "Epoch 179: train loss: 0.052339714020490646\n",
      "Epoch 180: train loss: 0.05233084782958031\n",
      "Epoch 181: train loss: 0.05232200771570206\n",
      "Epoch 182: train loss: 0.052313193678855896\n",
      "Epoch 183: train loss: 0.05230439081788063\n",
      "Epoch 184: train loss: 0.052295614033937454\n",
      "Epoch 185: train loss: 0.05228688195347786\n",
      "Epoch 186: train loss: 0.05227815732359886\n",
      "Epoch 187: train loss: 0.05226944759488106\n",
      "Epoch 188: train loss: 0.05226077884435654\n",
      "Epoch 189: train loss: 0.052252110093832016\n",
      "Epoch 190: train loss: 0.05224348232150078\n",
      "Epoch 191: train loss: 0.05223487317562103\n",
      "Epoch 192: train loss: 0.052226271480321884\n",
      "Epoch 193: train loss: 0.05221771076321602\n",
      "Epoch 194: train loss: 0.05220916122198105\n",
      "Epoch 195: train loss: 0.052200641483068466\n",
      "Epoch 196: train loss: 0.052192144095897675\n",
      "Epoch 197: train loss: 0.052183665335178375\n",
      "Epoch 198: train loss: 0.05217520892620087\n",
      "Epoch 199: train loss: 0.05216677486896515\n",
      "Epoch 200: train loss: 0.05215834826231003\n",
      "Epoch 201: train loss: 0.05214997008442879\n",
      "Epoch 202: train loss: 0.05214159935712814\n",
      "Epoch 203: train loss: 0.05213324725627899\n",
      "Epoch 204: train loss: 0.05212492123246193\n",
      "Epoch 205: train loss: 0.05211661756038666\n",
      "Epoch 206: train loss: 0.052108343690633774\n",
      "Epoch 207: train loss: 0.052100080996751785\n",
      "Epoch 208: train loss: 0.052091844379901886\n",
      "Epoch 209: train loss: 0.05208361893892288\n",
      "Epoch 210: train loss: 0.052075427025556564\n",
      "Epoch 211: train loss: 0.052067238837480545\n",
      "Epoch 212: train loss: 0.05205908417701721\n",
      "Epoch 213: train loss: 0.05205094814300537\n",
      "Epoch 214: train loss: 0.05204283446073532\n",
      "Epoch 215: train loss: 0.05203475058078766\n",
      "Epoch 216: train loss: 0.05202667415142059\n",
      "Epoch 217: train loss: 0.05201862007379532\n",
      "Epoch 218: train loss: 0.052010588347911835\n",
      "Epoch 219: train loss: 0.05200258642435074\n",
      "Epoch 220: train loss: 0.05199459567666054\n",
      "Epoch 221: train loss: 0.05198661983013153\n",
      "Epoch 222: train loss: 0.05197867006063461\n",
      "Epoch 223: train loss: 0.05197075381875038\n",
      "Epoch 224: train loss: 0.05196283385157585\n",
      "Epoch 225: train loss: 0.05195494368672371\n",
      "Epoch 226: train loss: 0.051947079598903656\n",
      "Epoch 227: train loss: 0.0519392266869545\n",
      "Epoch 228: train loss: 0.05193140730261803\n",
      "Epoch 229: train loss: 0.051923587918281555\n",
      "Epoch 230: train loss: 0.05191580951213837\n",
      "Epoch 231: train loss: 0.05190802738070488\n",
      "Epoch 232: train loss: 0.051900286227464676\n",
      "Epoch 233: train loss: 0.05189255252480507\n",
      "Epoch 234: train loss: 0.05188484489917755\n",
      "Epoch 235: train loss: 0.051877159625291824\n",
      "Epoch 236: train loss: 0.05186948552727699\n",
      "Epoch 237: train loss: 0.051861826330423355\n",
      "Epoch 238: train loss: 0.051854208111763\n",
      "Epoch 239: train loss: 0.05184660106897354\n",
      "Epoch 240: train loss: 0.051839012652635574\n",
      "Epoch 241: train loss: 0.05183141306042671\n",
      "Epoch 242: train loss: 0.05182386562228203\n",
      "Epoch 243: train loss: 0.05181632936000824\n",
      "Epoch 244: train loss: 0.05180882290005684\n",
      "Epoch 245: train loss: 0.05180131644010544\n",
      "Epoch 246: train loss: 0.05179384723305702\n",
      "Epoch 247: train loss: 0.05178637430071831\n",
      "Epoch 248: train loss: 0.05177894979715347\n",
      "Epoch 249: train loss: 0.05177152156829834\n",
      "Epoch 250: train loss: 0.051764119416475296\n",
      "Epoch 251: train loss: 0.05175671726465225\n",
      "Epoch 252: train loss: 0.05174936354160309\n",
      "Epoch 253: train loss: 0.05174202099442482\n",
      "Epoch 254: train loss: 0.051734697073698044\n",
      "Epoch 255: train loss: 0.05172736942768097\n",
      "Epoch 256: train loss: 0.051720090210437775\n",
      "Epoch 257: train loss: 0.05171280726790428\n",
      "Epoch 258: train loss: 0.05170556530356407\n",
      "Epoch 259: train loss: 0.05169832333922386\n",
      "Epoch 260: train loss: 0.05169110372662544\n",
      "Epoch 261: train loss: 0.051683906465768814\n",
      "Epoch 262: train loss: 0.05167672410607338\n",
      "Epoch 263: train loss: 0.05166954919695854\n",
      "Epoch 264: train loss: 0.05166240409016609\n",
      "Epoch 265: train loss: 0.051655273884534836\n",
      "Epoch 266: train loss: 0.05164817348122597\n",
      "Epoch 267: train loss: 0.051641084253787994\n",
      "Epoch 268: train loss: 0.05163399875164032\n",
      "Epoch 269: train loss: 0.05162694305181503\n",
      "Epoch 270: train loss: 0.05161990225315094\n",
      "Epoch 271: train loss: 0.05161288380622864\n",
      "Epoch 272: train loss: 0.05160588026046753\n",
      "Epoch 273: train loss: 0.05159888043999672\n",
      "Epoch 274: train loss: 0.051591917872428894\n",
      "Epoch 275: train loss: 0.05158497393131256\n",
      "Epoch 276: train loss: 0.051578033715486526\n",
      "Epoch 277: train loss: 0.051571108400821686\n",
      "Epoch 278: train loss: 0.051564209163188934\n",
      "Epoch 279: train loss: 0.05155731737613678\n",
      "Epoch 280: train loss: 0.051550451666116714\n",
      "Epoch 281: train loss: 0.051543597131967545\n",
      "Epoch 282: train loss: 0.051536768674850464\n",
      "Epoch 283: train loss: 0.051529958844184875\n",
      "Epoch 284: train loss: 0.05152316018939018\n",
      "Epoch 285: train loss: 0.05151638388633728\n",
      "Epoch 286: train loss: 0.051509611308574677\n",
      "Epoch 287: train loss: 0.051502861082553864\n",
      "Epoch 288: train loss: 0.05149613320827484\n",
      "Epoch 289: train loss: 0.05148942396044731\n",
      "Epoch 290: train loss: 0.05148271098732948\n",
      "Epoch 291: train loss: 0.05147603526711464\n",
      "Epoch 292: train loss: 0.05146936699748039\n",
      "Epoch 293: train loss: 0.051462724804878235\n",
      "Epoch 294: train loss: 0.05145607888698578\n",
      "Epoch 295: train loss: 0.05144946277141571\n",
      "Epoch 296: train loss: 0.05144287273287773\n",
      "Epoch 297: train loss: 0.05143628269433975\n",
      "Epoch 298: train loss: 0.051429715007543564\n",
      "Epoch 299: train loss: 0.05142316594719887\n",
      "Epoch 300: train loss: 0.05141662433743477\n",
      "Epoch 301: train loss: 0.05141010135412216\n",
      "Epoch 302: train loss: 0.05140360817313194\n",
      "Epoch 303: train loss: 0.05139712244272232\n",
      "Epoch 304: train loss: 0.05139065906405449\n",
      "Epoch 305: train loss: 0.05138419568538666\n",
      "Epoch 306: train loss: 0.05137775093317032\n",
      "Epoch 307: train loss: 0.05137133598327637\n",
      "Epoch 308: train loss: 0.05136492848396301\n",
      "Epoch 309: train loss: 0.05135853588581085\n",
      "Epoch 310: train loss: 0.05135215073823929\n",
      "Epoch 311: train loss: 0.051345791667699814\n",
      "Epoch 312: train loss: 0.05133944749832153\n",
      "Epoch 313: train loss: 0.051333118230104446\n",
      "Epoch 314: train loss: 0.05132679641246796\n",
      "Epoch 315: train loss: 0.05132049694657326\n",
      "Epoch 316: train loss: 0.05131421238183975\n",
      "Epoch 317: train loss: 0.05130794271826744\n",
      "Epoch 318: train loss: 0.05130169168114662\n",
      "Epoch 319: train loss: 0.05129546299576759\n",
      "Epoch 320: train loss: 0.051289234310388565\n",
      "Epoch 321: train loss: 0.051283013075590134\n",
      "Epoch 322: train loss: 0.05127682536840439\n",
      "Epoch 323: train loss: 0.051270656287670135\n",
      "Epoch 324: train loss: 0.05126449093222618\n",
      "Epoch 325: train loss: 0.051258333027362823\n",
      "Epoch 326: train loss: 0.051252204924821854\n",
      "Epoch 327: train loss: 0.05124609172344208\n",
      "Epoch 328: train loss: 0.0512399785220623\n",
      "Epoch 329: train loss: 0.05123390257358551\n",
      "Epoch 330: train loss: 0.05122781917452812\n",
      "Epoch 331: train loss: 0.05122176930308342\n",
      "Epoch 332: train loss: 0.05121571198105812\n",
      "Epoch 333: train loss: 0.051209691911935806\n",
      "Epoch 334: train loss: 0.05120368301868439\n",
      "Epoch 335: train loss: 0.05119767412543297\n",
      "Epoch 336: train loss: 0.051191698759794235\n",
      "Epoch 337: train loss: 0.051185715943574905\n",
      "Epoch 338: train loss: 0.051179759204387665\n",
      "Epoch 339: train loss: 0.05117382854223251\n",
      "Epoch 340: train loss: 0.05116788297891617\n",
      "Epoch 341: train loss: 0.05116197466850281\n",
      "Epoch 342: train loss: 0.05115608125925064\n",
      "Epoch 343: train loss: 0.05115018039941788\n",
      "Epoch 344: train loss: 0.0511443056166172\n",
      "Epoch 345: train loss: 0.05113845691084862\n",
      "Epoch 346: train loss: 0.05113260820508003\n",
      "Epoch 347: train loss: 0.05112677440047264\n",
      "Epoch 348: train loss: 0.05112096294760704\n",
      "Epoch 349: train loss: 0.051115166395902634\n",
      "Epoch 350: train loss: 0.05110936984419823\n",
      "Epoch 351: train loss: 0.05110360309481621\n",
      "Epoch 352: train loss: 0.05109784007072449\n",
      "Epoch 353: train loss: 0.05109209194779396\n",
      "Epoch 354: train loss: 0.051086362451314926\n",
      "Epoch 355: train loss: 0.051080647855997086\n",
      "Epoch 356: train loss: 0.05107492953538895\n",
      "Epoch 357: train loss: 0.05106924846768379\n",
      "Epoch 358: train loss: 0.05106355994939804\n",
      "Epoch 359: train loss: 0.05105789005756378\n",
      "Epoch 360: train loss: 0.05105223506689072\n",
      "Epoch 361: train loss: 0.051046598702669144\n",
      "Epoch 362: train loss: 0.051040973514318466\n",
      "Epoch 363: train loss: 0.05103537067770958\n",
      "Epoch 364: train loss: 0.05102977156639099\n",
      "Epoch 365: train loss: 0.051024194806814194\n",
      "Epoch 366: train loss: 0.0510186068713665\n",
      "Epoch 367: train loss: 0.051013052463531494\n",
      "Epoch 368: train loss: 0.051007505506277084\n",
      "Epoch 369: train loss: 0.051001980900764465\n",
      "Epoch 370: train loss: 0.05099646374583244\n",
      "Epoch 371: train loss: 0.05099094659090042\n",
      "Epoch 372: train loss: 0.05098545551300049\n",
      "Epoch 373: train loss: 0.05097997188568115\n",
      "Epoch 374: train loss: 0.050974514335393906\n",
      "Epoch 375: train loss: 0.05096905678510666\n",
      "Epoch 376: train loss: 0.05096361041069031\n",
      "Epoch 377: train loss: 0.05095818266272545\n",
      "Epoch 378: train loss: 0.050952766090631485\n",
      "Epoch 379: train loss: 0.05094737187027931\n",
      "Epoch 380: train loss: 0.05094197019934654\n",
      "Epoch 381: train loss: 0.05093660205602646\n",
      "Epoch 382: train loss: 0.05093123018741608\n",
      "Epoch 383: train loss: 0.05092588812112808\n",
      "Epoch 384: train loss: 0.05092053487896919\n",
      "Epoch 385: train loss: 0.05091521888971329\n",
      "Epoch 386: train loss: 0.050909899175167084\n",
      "Epoch 387: train loss: 0.050904594361782074\n",
      "Epoch 388: train loss: 0.05089930072426796\n",
      "Epoch 389: train loss: 0.050894029438495636\n",
      "Epoch 390: train loss: 0.05088876560330391\n",
      "Epoch 391: train loss: 0.05088350549340248\n",
      "Epoch 392: train loss: 0.05087827891111374\n",
      "Epoch 393: train loss: 0.0508730411529541\n",
      "Epoch 394: train loss: 0.05086781084537506\n",
      "Epoch 395: train loss: 0.05086261034011841\n",
      "Epoch 396: train loss: 0.05085741728544235\n",
      "Epoch 397: train loss: 0.05085223540663719\n",
      "Epoch 398: train loss: 0.05084707587957382\n",
      "Epoch 399: train loss: 0.05084192007780075\n",
      "Epoch 400: train loss: 0.05083677917718887\n",
      "Epoch 401: train loss: 0.05083164572715759\n",
      "Epoch 402: train loss: 0.05082652345299721\n",
      "Epoch 403: train loss: 0.05082141235470772\n",
      "Epoch 404: train loss: 0.050816308706998825\n",
      "Epoch 405: train loss: 0.05081123113632202\n",
      "Epoch 406: train loss: 0.05080615356564522\n",
      "Epoch 407: train loss: 0.05080109462141991\n",
      "Epoch 408: train loss: 0.05079604312777519\n",
      "Epoch 409: train loss: 0.05079101398587227\n",
      "Epoch 410: train loss: 0.05078597366809845\n",
      "Epoch 411: train loss: 0.05078095570206642\n",
      "Epoch 412: train loss: 0.050775960087776184\n",
      "Epoch 413: train loss: 0.050770968198776245\n",
      "Epoch 414: train loss: 0.0507659837603569\n",
      "Epoch 415: train loss: 0.05076102167367935\n",
      "Epoch 416: train loss: 0.050756052136421204\n",
      "Epoch 417: train loss: 0.05075111240148544\n",
      "Epoch 418: train loss: 0.05074617639183998\n",
      "Epoch 419: train loss: 0.05074125900864601\n",
      "Epoch 420: train loss: 0.05073634162545204\n",
      "Epoch 421: train loss: 0.05073143541812897\n",
      "Epoch 422: train loss: 0.05072655528783798\n",
      "Epoch 423: train loss: 0.050721678882837296\n",
      "Epoch 424: train loss: 0.05071680620312691\n",
      "Epoch 425: train loss: 0.05071195960044861\n",
      "Epoch 426: train loss: 0.05070710554718971\n",
      "Epoch 427: train loss: 0.05070227384567261\n",
      "Epoch 428: train loss: 0.0506974458694458\n",
      "Epoch 429: train loss: 0.050692636519670486\n",
      "Epoch 430: train loss: 0.05068783834576607\n",
      "Epoch 431: train loss: 0.050683047622442245\n",
      "Epoch 432: train loss: 0.05067826434969902\n",
      "Epoch 433: train loss: 0.05067349970340729\n",
      "Epoch 434: train loss: 0.05066875368356705\n",
      "Epoch 435: train loss: 0.05066400021314621\n",
      "Epoch 436: train loss: 0.05065925419330597\n",
      "Epoch 437: train loss: 0.05065453052520752\n",
      "Epoch 438: train loss: 0.05064982548356056\n",
      "Epoch 439: train loss: 0.050645116716623306\n",
      "Epoch 440: train loss: 0.050640419125556946\n",
      "Epoch 441: train loss: 0.05063573643565178\n",
      "Epoch 442: train loss: 0.05063106119632721\n",
      "Epoch 443: train loss: 0.05062641203403473\n",
      "Epoch 444: train loss: 0.050621747970581055\n",
      "Epoch 445: train loss: 0.05061710625886917\n",
      "Epoch 446: train loss: 0.05061248317360878\n",
      "Epoch 447: train loss: 0.05060786381363869\n",
      "Epoch 448: train loss: 0.05060325190424919\n",
      "Epoch 449: train loss: 0.05059865489602089\n",
      "Epoch 450: train loss: 0.05059405416250229\n",
      "Epoch 451: train loss: 0.05058949068188667\n",
      "Epoch 452: train loss: 0.05058491230010986\n",
      "Epoch 453: train loss: 0.05058035999536514\n",
      "Epoch 454: train loss: 0.05057581514120102\n",
      "Epoch 455: train loss: 0.05057127773761749\n",
      "Epoch 456: train loss: 0.05056674778461456\n",
      "Epoch 457: train loss: 0.05056222900748253\n",
      "Epoch 458: train loss: 0.050557732582092285\n",
      "Epoch 459: train loss: 0.050553228706121445\n",
      "Epoch 460: train loss: 0.0505487360060215\n",
      "Epoch 461: train loss: 0.050544265657663345\n",
      "Epoch 462: train loss: 0.05053979158401489\n",
      "Epoch 463: train loss: 0.05053534358739853\n",
      "Epoch 464: train loss: 0.05053089186549187\n",
      "Epoch 465: train loss: 0.0505264587700367\n",
      "Epoch 466: train loss: 0.050522033125162125\n",
      "Epoch 467: train loss: 0.05051761865615845\n",
      "Epoch 468: train loss: 0.05051320418715477\n",
      "Epoch 469: train loss: 0.050508804619312286\n",
      "Epoch 470: train loss: 0.0505044162273407\n",
      "Epoch 471: train loss: 0.05050003528594971\n",
      "Epoch 472: train loss: 0.05049566924571991\n",
      "Epoch 473: train loss: 0.05049131065607071\n",
      "Epoch 474: train loss: 0.0504869669675827\n",
      "Epoch 475: train loss: 0.050482623279094696\n",
      "Epoch 476: train loss: 0.050478287041187286\n",
      "Epoch 477: train loss: 0.05047396197915077\n",
      "Epoch 478: train loss: 0.05046965554356575\n",
      "Epoch 479: train loss: 0.050465356558561325\n",
      "Epoch 480: train loss: 0.05046107620000839\n",
      "Epoch 481: train loss: 0.05045677348971367\n",
      "Epoch 482: train loss: 0.05045251175761223\n",
      "Epoch 483: train loss: 0.05044823884963989\n",
      "Epoch 484: train loss: 0.05044398456811905\n",
      "Epoch 485: train loss: 0.0504397489130497\n",
      "Epoch 486: train loss: 0.05043550953269005\n",
      "Epoch 487: train loss: 0.050431281328201294\n",
      "Epoch 488: train loss: 0.05042706057429314\n",
      "Epoch 489: train loss: 0.050422850996255875\n",
      "Epoch 490: train loss: 0.05041864886879921\n",
      "Epoch 491: train loss: 0.050414472818374634\n",
      "Epoch 492: train loss: 0.05041028931736946\n",
      "Epoch 493: train loss: 0.05040612071752548\n",
      "Epoch 494: train loss: 0.050401948392391205\n",
      "Epoch 495: train loss: 0.05039779841899872\n",
      "Epoch 496: train loss: 0.05039365589618683\n",
      "Epoch 497: train loss: 0.05038951709866524\n",
      "Epoch 498: train loss: 0.05038538947701454\n",
      "Epoch 499: train loss: 0.05038127303123474\n",
      "Epoch 500: train loss: 0.05037717893719673\n",
      "Epoch 501: train loss: 0.05037306621670723\n",
      "Epoch 502: train loss: 0.050368983298540115\n",
      "Epoch 503: train loss: 0.0503648966550827\n",
      "Epoch 504: train loss: 0.05036083608865738\n",
      "Epoch 505: train loss: 0.05035675689578056\n",
      "Epoch 506: train loss: 0.050352707505226135\n",
      "Epoch 507: train loss: 0.050348661839962006\n",
      "Epoch 508: train loss: 0.05034462735056877\n",
      "Epoch 509: train loss: 0.05034059286117554\n",
      "Epoch 510: train loss: 0.0503365658223629\n",
      "Epoch 511: train loss: 0.05033256486058235\n",
      "Epoch 512: train loss: 0.050328560173511505\n",
      "Epoch 513: train loss: 0.05032457038760185\n",
      "Epoch 514: train loss: 0.0503205768764019\n",
      "Epoch 515: train loss: 0.05031660199165344\n",
      "Epoch 516: train loss: 0.050312623381614685\n",
      "Epoch 517: train loss: 0.05030867084860802\n",
      "Epoch 518: train loss: 0.05030472204089165\n",
      "Epoch 519: train loss: 0.050300776958465576\n",
      "Epoch 520: train loss: 0.0502968430519104\n",
      "Epoch 521: train loss: 0.05029292404651642\n",
      "Epoch 522: train loss: 0.05028900131583214\n",
      "Epoch 523: train loss: 0.05028509348630905\n",
      "Epoch 524: train loss: 0.05028118938207626\n",
      "Epoch 525: train loss: 0.05027729272842407\n",
      "Epoch 526: train loss: 0.050273410975933075\n",
      "Epoch 527: train loss: 0.050269532948732376\n",
      "Epoch 528: train loss: 0.05026565492153168\n",
      "Epoch 529: train loss: 0.050261810421943665\n",
      "Epoch 530: train loss: 0.050257954746484756\n",
      "Epoch 531: train loss: 0.050254106521606445\n",
      "Epoch 532: train loss: 0.05025026574730873\n",
      "Epoch 533: train loss: 0.05024644732475281\n",
      "Epoch 534: train loss: 0.050242628902196884\n",
      "Epoch 535: train loss: 0.05023881793022156\n",
      "Epoch 536: train loss: 0.05023501440882683\n",
      "Epoch 537: train loss: 0.050231222063302994\n",
      "Epoch 538: train loss: 0.050227437168359756\n",
      "Epoch 539: train loss: 0.05022365599870682\n",
      "Epoch 540: train loss: 0.05021989345550537\n",
      "Epoch 541: train loss: 0.05021611973643303\n",
      "Epoch 542: train loss: 0.050212372094392776\n",
      "Epoch 543: train loss: 0.050208620727062225\n",
      "Epoch 544: train loss: 0.05020487681031227\n",
      "Epoch 545: train loss: 0.05020114406943321\n",
      "Epoch 546: train loss: 0.05019742622971535\n",
      "Epoch 547: train loss: 0.05019371211528778\n",
      "Epoch 548: train loss: 0.05019000545144081\n",
      "Epoch 549: train loss: 0.05018630251288414\n",
      "Epoch 550: train loss: 0.05018259957432747\n",
      "Epoch 551: train loss: 0.05017891898751259\n",
      "Epoch 552: train loss: 0.050175245851278305\n",
      "Epoch 553: train loss: 0.05017157644033432\n",
      "Epoch 554: train loss: 0.050167907029390335\n",
      "Epoch 555: train loss: 0.05016425624489784\n",
      "Epoch 556: train loss: 0.05016061291098595\n",
      "Epoch 557: train loss: 0.05015697330236435\n",
      "Epoch 558: train loss: 0.05015334486961365\n",
      "Epoch 559: train loss: 0.050149716436862946\n",
      "Epoch 560: train loss: 0.050146106630563736\n",
      "Epoch 561: train loss: 0.050142496824264526\n",
      "Epoch 562: train loss: 0.05013890936970711\n",
      "Epoch 563: train loss: 0.050135303288698196\n",
      "Epoch 564: train loss: 0.05013170465826988\n",
      "Epoch 565: train loss: 0.05012813210487366\n",
      "Epoch 566: train loss: 0.05012456700205803\n",
      "Epoch 567: train loss: 0.050120994448661804\n",
      "Epoch 568: train loss: 0.05011744424700737\n",
      "Epoch 569: train loss: 0.05011388659477234\n",
      "Epoch 570: train loss: 0.0501103512942791\n",
      "Epoch 571: train loss: 0.050106823444366455\n",
      "Epoch 572: train loss: 0.05010329559445381\n",
      "Epoch 573: train loss: 0.05009976029396057\n",
      "Epoch 574: train loss: 0.05009625479578972\n",
      "Epoch 575: train loss: 0.05009274184703827\n",
      "Epoch 576: train loss: 0.05008925497531891\n",
      "Epoch 577: train loss: 0.05008575692772865\n",
      "Epoch 578: train loss: 0.05008228123188019\n",
      "Epoch 579: train loss: 0.050078801810741425\n",
      "Epoch 580: train loss: 0.050075333565473557\n",
      "Epoch 581: train loss: 0.05007186904549599\n",
      "Epoch 582: train loss: 0.05006841570138931\n",
      "Epoch 583: train loss: 0.05006496608257294\n",
      "Epoch 584: train loss: 0.05006152763962746\n",
      "Epoch 585: train loss: 0.050058089196681976\n",
      "Epoch 586: train loss: 0.050054654479026794\n",
      "Epoch 587: train loss: 0.050051238387823105\n",
      "Epoch 588: train loss: 0.050047826021909714\n",
      "Epoch 589: train loss: 0.05004441365599632\n",
      "Epoch 590: train loss: 0.050041019916534424\n",
      "Epoch 591: train loss: 0.050037626177072525\n",
      "Epoch 592: train loss: 0.050034232437610626\n",
      "Epoch 593: train loss: 0.05003085359930992\n",
      "Epoch 594: train loss: 0.05002748593688011\n",
      "Epoch 595: train loss: 0.050024114549160004\n",
      "Epoch 596: train loss: 0.05002076178789139\n",
      "Epoch 597: train loss: 0.050017401576042175\n",
      "Epoch 598: train loss: 0.05001406371593475\n",
      "Epoch 599: train loss: 0.05001072585582733\n",
      "Epoch 600: train loss: 0.05000739172101021\n",
      "Epoch 601: train loss: 0.05000406503677368\n",
      "Epoch 602: train loss: 0.05000075325369835\n",
      "Epoch 603: train loss: 0.04999743402004242\n",
      "Epoch 604: train loss: 0.049994129687547684\n",
      "Epoch 605: train loss: 0.04999083653092384\n",
      "Epoch 606: train loss: 0.0499875433743\n",
      "Epoch 607: train loss: 0.04998425021767616\n",
      "Epoch 608: train loss: 0.04998098313808441\n",
      "Epoch 609: train loss: 0.049977708607912064\n",
      "Epoch 610: train loss: 0.04997444152832031\n",
      "Epoch 611: train loss: 0.04997117817401886\n",
      "Epoch 612: train loss: 0.0499679297208786\n",
      "Epoch 613: train loss: 0.04996468499302864\n",
      "Epoch 614: train loss: 0.04996144399046898\n",
      "Epoch 615: train loss: 0.049958206713199615\n",
      "Epoch 616: train loss: 0.049954984337091446\n",
      "Epoch 617: train loss: 0.04995176941156387\n",
      "Epoch 618: train loss: 0.049948550760746\n",
      "Epoch 619: train loss: 0.049945347011089325\n",
      "Epoch 620: train loss: 0.04994213953614235\n",
      "Epoch 621: train loss: 0.04993894323706627\n",
      "Epoch 622: train loss: 0.04993576928973198\n",
      "Epoch 623: train loss: 0.049932580441236496\n",
      "Epoch 624: train loss: 0.04992940276861191\n",
      "Epoch 625: train loss: 0.04992622882127762\n",
      "Epoch 626: train loss: 0.04992305859923363\n",
      "Epoch 627: train loss: 0.04991991072893143\n",
      "Epoch 628: train loss: 0.04991676285862923\n",
      "Epoch 629: train loss: 0.04991361126303673\n",
      "Epoch 630: train loss: 0.04991047456860542\n",
      "Epoch 631: train loss: 0.049907345324754715\n",
      "Epoch 632: train loss: 0.049904219806194305\n",
      "Epoch 633: train loss: 0.049901098012924194\n",
      "Epoch 634: train loss: 0.04989797621965408\n",
      "Epoch 635: train loss: 0.049894873052835464\n",
      "Epoch 636: train loss: 0.049891769886016846\n",
      "Epoch 637: train loss: 0.049888674169778824\n",
      "Epoch 638: train loss: 0.0498855784535408\n",
      "Epoch 639: train loss: 0.049882493913173676\n",
      "Epoch 640: train loss: 0.04987943172454834\n",
      "Epoch 641: train loss: 0.04987635836005211\n",
      "Epoch 642: train loss: 0.049873288720846176\n",
      "Epoch 643: train loss: 0.04987023398280144\n",
      "Epoch 644: train loss: 0.0498671755194664\n",
      "Epoch 645: train loss: 0.04986412823200226\n",
      "Epoch 646: train loss: 0.04986107721924782\n",
      "Epoch 647: train loss: 0.04985805228352547\n",
      "Epoch 648: train loss: 0.04985501244664192\n",
      "Epoch 649: train loss: 0.04985199123620987\n",
      "Epoch 650: train loss: 0.04984896630048752\n",
      "Epoch 651: train loss: 0.04984596371650696\n",
      "Epoch 652: train loss: 0.0498429536819458\n",
      "Epoch 653: train loss: 0.049839962273836136\n",
      "Epoch 654: train loss: 0.04983694851398468\n",
      "Epoch 655: train loss: 0.049833979457616806\n",
      "Epoch 656: train loss: 0.04983098432421684\n",
      "Epoch 657: train loss: 0.04982801154255867\n",
      "Epoch 658: train loss: 0.0498250350356102\n",
      "Epoch 659: train loss: 0.04982206970453262\n",
      "Epoch 660: train loss: 0.049819108098745346\n",
      "Epoch 661: train loss: 0.04981614276766777\n",
      "Epoch 662: train loss: 0.04981321096420288\n",
      "Epoch 663: train loss: 0.0498102568089962\n",
      "Epoch 664: train loss: 0.049807317554950714\n",
      "Epoch 665: train loss: 0.04980439320206642\n",
      "Epoch 666: train loss: 0.049801457673311234\n",
      "Epoch 667: train loss: 0.04979853704571724\n",
      "Epoch 668: train loss: 0.04979562759399414\n",
      "Epoch 669: train loss: 0.04979271441698074\n",
      "Epoch 670: train loss: 0.049789804965257645\n",
      "Epoch 671: train loss: 0.04978690668940544\n",
      "Epoch 672: train loss: 0.04978401958942413\n",
      "Epoch 673: train loss: 0.04978112876415253\n",
      "Epoch 674: train loss: 0.04977824166417122\n",
      "Epoch 675: train loss: 0.049775365740060806\n",
      "Epoch 676: train loss: 0.049772489815950394\n",
      "Epoch 677: train loss: 0.049769625067710876\n",
      "Epoch 678: train loss: 0.049766767770051956\n",
      "Epoch 679: train loss: 0.049763910472393036\n",
      "Epoch 680: train loss: 0.04976106807589531\n",
      "Epoch 681: train loss: 0.04975820705294609\n",
      "Epoch 682: train loss: 0.04975537583231926\n",
      "Epoch 683: train loss: 0.04975254833698273\n",
      "Epoch 684: train loss: 0.049749705940485\n",
      "Epoch 685: train loss: 0.04974689334630966\n",
      "Epoch 686: train loss: 0.04974406957626343\n",
      "Epoch 687: train loss: 0.049741264432668686\n",
      "Epoch 688: train loss: 0.04973844066262245\n",
      "Epoch 689: train loss: 0.04973564296960831\n",
      "Epoch 690: train loss: 0.04973285272717476\n",
      "Epoch 691: train loss: 0.04973006621003151\n",
      "Epoch 692: train loss: 0.049727264791727066\n",
      "Epoch 693: train loss: 0.049724481999874115\n",
      "Epoch 694: train loss: 0.04972170665860176\n",
      "Epoch 695: train loss: 0.049718938767910004\n",
      "Epoch 696: train loss: 0.049716170877218246\n",
      "Epoch 697: train loss: 0.049713414162397385\n",
      "Epoch 698: train loss: 0.049710653722286224\n",
      "Epoch 699: train loss: 0.04970790073275566\n",
      "Epoch 700: train loss: 0.04970515891909599\n",
      "Epoch 701: train loss: 0.049702420830726624\n",
      "Epoch 702: train loss: 0.049699682742357254\n",
      "Epoch 703: train loss: 0.04969695582985878\n",
      "Epoch 704: train loss: 0.04969422519207001\n",
      "Epoch 705: train loss: 0.04969150200486183\n",
      "Epoch 706: train loss: 0.04968879371881485\n",
      "Epoch 707: train loss: 0.049686089158058167\n",
      "Epoch 708: train loss: 0.049683380872011185\n",
      "Epoch 709: train loss: 0.0496806725859642\n",
      "Epoch 710: train loss: 0.049677979201078415\n",
      "Epoch 711: train loss: 0.049675293266773224\n",
      "Epoch 712: train loss: 0.049672603607177734\n",
      "Epoch 713: train loss: 0.04966992139816284\n",
      "Epoch 714: train loss: 0.04966725781559944\n",
      "Epoch 715: train loss: 0.04966457933187485\n",
      "Epoch 716: train loss: 0.04966190829873085\n",
      "Epoch 717: train loss: 0.049659255892038345\n",
      "Epoch 718: train loss: 0.049656592309474945\n",
      "Epoch 719: train loss: 0.04965393990278244\n",
      "Epoch 720: train loss: 0.049651291221380234\n",
      "Epoch 721: train loss: 0.049648649990558624\n",
      "Epoch 722: train loss: 0.04964601993560791\n",
      "Epoch 723: train loss: 0.049643389880657196\n",
      "Epoch 724: train loss: 0.049640752375125885\n",
      "Epoch 725: train loss: 0.04963814094662666\n",
      "Epoch 726: train loss: 0.049635518342256546\n",
      "Epoch 727: train loss: 0.049632906913757324\n",
      "Epoch 728: train loss: 0.0496303029358387\n",
      "Epoch 729: train loss: 0.04962768405675888\n",
      "Epoch 730: train loss: 0.04962509125471115\n",
      "Epoch 731: train loss: 0.04962249845266342\n",
      "Epoch 732: train loss: 0.04961990565061569\n",
      "Epoch 733: train loss: 0.04961732402443886\n",
      "Epoch 734: train loss: 0.049614742398262024\n",
      "Epoch 735: train loss: 0.04961216449737549\n",
      "Epoch 736: train loss: 0.04960958659648895\n",
      "Epoch 737: train loss: 0.04960703104734421\n",
      "Epoch 738: train loss: 0.049604468047618866\n",
      "Epoch 739: train loss: 0.04960191249847412\n",
      "Epoch 740: train loss: 0.04959935322403908\n",
      "Epoch 741: train loss: 0.04959680512547493\n",
      "Epoch 742: train loss: 0.04959425702691078\n",
      "Epoch 743: train loss: 0.04959172382950783\n",
      "Epoch 744: train loss: 0.049589190632104874\n",
      "Epoch 745: train loss: 0.04958665370941162\n",
      "Epoch 746: train loss: 0.04958413541316986\n",
      "Epoch 747: train loss: 0.0495816133916378\n",
      "Epoch 748: train loss: 0.04957910254597664\n",
      "Epoch 749: train loss: 0.04957658052444458\n",
      "Epoch 750: train loss: 0.049574077129364014\n",
      "Epoch 751: train loss: 0.04957157000899315\n",
      "Epoch 752: train loss: 0.04956907406449318\n",
      "Epoch 753: train loss: 0.04956658184528351\n",
      "Epoch 754: train loss: 0.049564097076654434\n",
      "Epoch 755: train loss: 0.049561597406864166\n",
      "Epoch 756: train loss: 0.04955911636352539\n",
      "Epoch 757: train loss: 0.04955664649605751\n",
      "Epoch 758: train loss: 0.04955418035387993\n",
      "Epoch 759: train loss: 0.04955170303583145\n",
      "Epoch 760: train loss: 0.04954924061894417\n",
      "Epoch 761: train loss: 0.049546778202056885\n",
      "Epoch 762: train loss: 0.0495443232357502\n",
      "Epoch 763: train loss: 0.04954187199473381\n",
      "Epoch 764: train loss: 0.04953942820429802\n",
      "Epoch 765: train loss: 0.04953698441386223\n",
      "Epoch 766: train loss: 0.049534544348716736\n",
      "Epoch 767: train loss: 0.04953211545944214\n",
      "Epoch 768: train loss: 0.04952968657016754\n",
      "Epoch 769: train loss: 0.04952726140618324\n",
      "Epoch 770: train loss: 0.04952484741806984\n",
      "Epoch 771: train loss: 0.04952241852879524\n",
      "Epoch 772: train loss: 0.04952000826597214\n",
      "Epoch 773: train loss: 0.04951759800314903\n",
      "Epoch 774: train loss: 0.049515191465616226\n",
      "Epoch 775: train loss: 0.049512796103954315\n",
      "Epoch 776: train loss: 0.049510397017002106\n",
      "Epoch 777: train loss: 0.049508001655340195\n",
      "Epoch 778: train loss: 0.04950561746954918\n",
      "Epoch 779: train loss: 0.049503225833177567\n",
      "Epoch 780: train loss: 0.049500852823257446\n",
      "Epoch 781: train loss: 0.049498483538627625\n",
      "Epoch 782: train loss: 0.049496110528707504\n",
      "Epoch 783: train loss: 0.049493733793497086\n",
      "Epoch 784: train loss: 0.04949137568473816\n",
      "Epoch 785: train loss: 0.04948902130126953\n",
      "Epoch 786: train loss: 0.049486663192510605\n",
      "Epoch 787: train loss: 0.04948430880904198\n",
      "Epoch 788: train loss: 0.049481965601444244\n",
      "Epoch 789: train loss: 0.04947961866855621\n",
      "Epoch 790: train loss: 0.04947727173566818\n",
      "Epoch 791: train loss: 0.049474943429231644\n",
      "Epoch 792: train loss: 0.04947260767221451\n",
      "Epoch 793: train loss: 0.04947028309106827\n",
      "Epoch 794: train loss: 0.049467965960502625\n",
      "Epoch 795: train loss: 0.04946564510464668\n",
      "Epoch 796: train loss: 0.04946332797408104\n",
      "Epoch 797: train loss: 0.04946102574467659\n",
      "Epoch 798: train loss: 0.04945871978998184\n",
      "Epoch 799: train loss: 0.049456413835287094\n",
      "Epoch 800: train loss: 0.04945411533117294\n",
      "Epoch 801: train loss: 0.04945181682705879\n",
      "Epoch 802: train loss: 0.04944951832294464\n",
      "Epoch 803: train loss: 0.04944723844528198\n",
      "Epoch 804: train loss: 0.04944494366645813\n",
      "Epoch 805: train loss: 0.04944266751408577\n",
      "Epoch 806: train loss: 0.04944038763642311\n",
      "Epoch 807: train loss: 0.04943811520934105\n",
      "Epoch 808: train loss: 0.049435846507549286\n",
      "Epoch 809: train loss: 0.04943358525633812\n",
      "Epoch 810: train loss: 0.049431320279836655\n",
      "Epoch 811: train loss: 0.049429066479206085\n",
      "Epoch 812: train loss: 0.049426816403865814\n",
      "Epoch 813: train loss: 0.049424558877944946\n",
      "Epoch 814: train loss: 0.04942232370376587\n",
      "Epoch 815: train loss: 0.0494200736284256\n",
      "Epoch 816: train loss: 0.04941783472895622\n",
      "Epoch 817: train loss: 0.049415599554777145\n",
      "Epoch 818: train loss: 0.049413371831178665\n",
      "Epoch 819: train loss: 0.049411140382289886\n",
      "Epoch 820: train loss: 0.049408912658691406\n",
      "Epoch 821: train loss: 0.04940670356154442\n",
      "Epoch 822: train loss: 0.04940447956323624\n",
      "Epoch 823: train loss: 0.04940227046608925\n",
      "Epoch 824: train loss: 0.04940005764365196\n",
      "Epoch 825: train loss: 0.04939785227179527\n",
      "Epoch 826: train loss: 0.04939564689993858\n",
      "Epoch 827: train loss: 0.049393460154533386\n",
      "Epoch 828: train loss: 0.0493912510573864\n",
      "Epoch 829: train loss: 0.0493890680372715\n",
      "Epoch 830: train loss: 0.0493868812918663\n",
      "Epoch 831: train loss: 0.049384698271751404\n",
      "Epoch 832: train loss: 0.049382518976926804\n",
      "Epoch 833: train loss: 0.0493803396821022\n",
      "Epoch 834: train loss: 0.0493781641125679\n",
      "Epoch 835: train loss: 0.049375999718904495\n",
      "Epoch 836: train loss: 0.04937382787466049\n",
      "Epoch 837: train loss: 0.04937167093157768\n",
      "Epoch 838: train loss: 0.04936950281262398\n",
      "Epoch 839: train loss: 0.04936734586954117\n",
      "Epoch 840: train loss: 0.04936520382761955\n",
      "Epoch 841: train loss: 0.04936305806040764\n",
      "Epoch 842: train loss: 0.049360912293195724\n",
      "Epoch 843: train loss: 0.04935877025127411\n",
      "Epoch 844: train loss: 0.04935662820935249\n",
      "Epoch 845: train loss: 0.04935448616743088\n",
      "Epoch 846: train loss: 0.04935235530138016\n",
      "Epoch 847: train loss: 0.04935023933649063\n",
      "Epoch 848: train loss: 0.04934810474514961\n",
      "Epoch 849: train loss: 0.04934598132967949\n",
      "Epoch 850: train loss: 0.04934386536478996\n",
      "Epoch 851: train loss: 0.04934176057577133\n",
      "Epoch 852: train loss: 0.04933963716030121\n",
      "Epoch 853: train loss: 0.04933754354715347\n",
      "Epoch 854: train loss: 0.04933543503284454\n",
      "Epoch 855: train loss: 0.04933333024382591\n",
      "Epoch 856: train loss: 0.04933123663067818\n",
      "Epoch 857: train loss: 0.04932914674282074\n",
      "Epoch 858: train loss: 0.0493270568549633\n",
      "Epoch 859: train loss: 0.04932495951652527\n",
      "Epoch 860: train loss: 0.049322884529829025\n",
      "Epoch 861: train loss: 0.049320802092552185\n",
      "Epoch 862: train loss: 0.04931872710585594\n",
      "Epoch 863: train loss: 0.0493166521191597\n",
      "Epoch 864: train loss: 0.04931458830833435\n",
      "Epoch 865: train loss: 0.049312520772218704\n",
      "Epoch 866: train loss: 0.049310456961393356\n",
      "Epoch 867: train loss: 0.04930838197469711\n",
      "Epoch 868: train loss: 0.049306340515613556\n",
      "Epoch 869: train loss: 0.049304280430078506\n",
      "Epoch 870: train loss: 0.04930223897099495\n",
      "Epoch 871: train loss: 0.0493001826107502\n",
      "Epoch 872: train loss: 0.04929813742637634\n",
      "Epoch 873: train loss: 0.049296099692583084\n",
      "Epoch 874: train loss: 0.049294065684080124\n",
      "Epoch 875: train loss: 0.04929202049970627\n",
      "Epoch 876: train loss: 0.049289997667074203\n",
      "Epoch 877: train loss: 0.04928796738386154\n",
      "Epoch 878: train loss: 0.04928594455122948\n",
      "Epoch 879: train loss: 0.049283917993307114\n",
      "Epoch 880: train loss: 0.04928191006183624\n",
      "Epoch 881: train loss: 0.04927988722920418\n",
      "Epoch 882: train loss: 0.04927787184715271\n",
      "Epoch 883: train loss: 0.04927586391568184\n",
      "Epoch 884: train loss: 0.049273859709501266\n",
      "Epoch 885: train loss: 0.04927186295390129\n",
      "Epoch 886: train loss: 0.04926985502243042\n",
      "Epoch 887: train loss: 0.04926786571741104\n",
      "Epoch 888: train loss: 0.04926585778594017\n",
      "Epoch 889: train loss: 0.04926386848092079\n",
      "Epoch 890: train loss: 0.04926188662648201\n",
      "Epoch 891: train loss: 0.04925990104675293\n",
      "Epoch 892: train loss: 0.04925791919231415\n",
      "Epoch 893: train loss: 0.04925594851374626\n",
      "Epoch 894: train loss: 0.04925396665930748\n",
      "Epoch 895: train loss: 0.04925199970602989\n",
      "Epoch 896: train loss: 0.049250029027462006\n",
      "Epoch 897: train loss: 0.04924807697534561\n",
      "Epoch 898: train loss: 0.04924610257148743\n",
      "Epoch 899: train loss: 0.049244143068790436\n",
      "Epoch 900: train loss: 0.049242183566093445\n",
      "Epoch 901: train loss: 0.04924023151397705\n",
      "Epoch 902: train loss: 0.04923827946186066\n",
      "Epoch 903: train loss: 0.04923632740974426\n",
      "Epoch 904: train loss: 0.049234382808208466\n",
      "Epoch 905: train loss: 0.04923243820667267\n",
      "Epoch 906: train loss: 0.04923050478100777\n",
      "Epoch 907: train loss: 0.04922855272889137\n",
      "Epoch 908: train loss: 0.049226630479097366\n",
      "Epoch 909: train loss: 0.04922470077872276\n",
      "Epoch 910: train loss: 0.04922276735305786\n",
      "Epoch 911: train loss: 0.049220845103263855\n",
      "Epoch 912: train loss: 0.04921892657876015\n",
      "Epoch 913: train loss: 0.049217015504837036\n",
      "Epoch 914: train loss: 0.049215104430913925\n",
      "Epoch 915: train loss: 0.049213189631700516\n",
      "Epoch 916: train loss: 0.049211278557777405\n",
      "Epoch 917: train loss: 0.049209367483854294\n",
      "Epoch 918: train loss: 0.04920746386051178\n",
      "Epoch 919: train loss: 0.049205563962459564\n",
      "Epoch 920: train loss: 0.04920365661382675\n",
      "Epoch 921: train loss: 0.04920176789164543\n",
      "Epoch 922: train loss: 0.049199871718883514\n",
      "Epoch 923: train loss: 0.04919798672199249\n",
      "Epoch 924: train loss: 0.04919610172510147\n",
      "Epoch 925: train loss: 0.04919421300292015\n",
      "Epoch 926: train loss: 0.049192339181900024\n",
      "Epoch 927: train loss: 0.049190450459718704\n",
      "Epoch 928: train loss: 0.049188580363988876\n",
      "Epoch 929: train loss: 0.04918670281767845\n",
      "Epoch 930: train loss: 0.04918484017252922\n",
      "Epoch 931: train loss: 0.049182966351509094\n",
      "Epoch 932: train loss: 0.04918111115694046\n",
      "Epoch 933: train loss: 0.04917924106121063\n",
      "Epoch 934: train loss: 0.049177393317222595\n",
      "Epoch 935: train loss: 0.04917553439736366\n",
      "Epoch 936: train loss: 0.04917367547750473\n",
      "Epoch 937: train loss: 0.049171820282936096\n",
      "Epoch 938: train loss: 0.049169979989528656\n",
      "Epoch 939: train loss: 0.049168143421411514\n",
      "Epoch 940: train loss: 0.04916629195213318\n",
      "Epoch 941: train loss: 0.049164436757564545\n",
      "Epoch 942: train loss: 0.0491626150906086\n",
      "Epoch 943: train loss: 0.04916077479720116\n",
      "Epoch 944: train loss: 0.04915894940495491\n",
      "Epoch 945: train loss: 0.04915711283683777\n",
      "Epoch 946: train loss: 0.04915529116988182\n",
      "Epoch 947: train loss: 0.04915347322821617\n",
      "Epoch 948: train loss: 0.04915165528655052\n",
      "Epoch 949: train loss: 0.049149829894304276\n",
      "Epoch 950: train loss: 0.049148015677928925\n",
      "Epoch 951: train loss: 0.04914619028568268\n",
      "Epoch 952: train loss: 0.04914439097046852\n",
      "Epoch 953: train loss: 0.049142587929964066\n",
      "Epoch 954: train loss: 0.04914078488945961\n",
      "Epoch 955: train loss: 0.04913897439837456\n",
      "Epoch 956: train loss: 0.0491371750831604\n",
      "Epoch 957: train loss: 0.04913537949323654\n",
      "Epoch 958: train loss: 0.04913358762860298\n",
      "Epoch 959: train loss: 0.049131788313388824\n",
      "Epoch 960: train loss: 0.04913000762462616\n",
      "Epoch 961: train loss: 0.049128223210573196\n",
      "Epoch 962: train loss: 0.04912644252181053\n",
      "Epoch 963: train loss: 0.04912465065717697\n",
      "Epoch 964: train loss: 0.04912286996841431\n",
      "Epoch 965: train loss: 0.04912110045552254\n",
      "Epoch 966: train loss: 0.049119316041469574\n",
      "Epoch 967: train loss: 0.0491175502538681\n",
      "Epoch 968: train loss: 0.04911578446626663\n",
      "Epoch 969: train loss: 0.04911401867866516\n",
      "Epoch 970: train loss: 0.04911225661635399\n",
      "Epoch 971: train loss: 0.04911048710346222\n",
      "Epoch 972: train loss: 0.04910873621702194\n",
      "Epoch 973: train loss: 0.04910697788000107\n",
      "Epoch 974: train loss: 0.049105219542980194\n",
      "Epoch 975: train loss: 0.04910346493124962\n",
      "Epoch 976: train loss: 0.049101728945970535\n",
      "Epoch 977: train loss: 0.04909997805953026\n",
      "Epoch 978: train loss: 0.049098238348960876\n",
      "Epoch 979: train loss: 0.049096494913101196\n",
      "Epoch 980: train loss: 0.04909475892782211\n",
      "Epoch 981: train loss: 0.04909301921725273\n",
      "Epoch 982: train loss: 0.04909128323197365\n",
      "Epoch 983: train loss: 0.04908955842256546\n",
      "Epoch 984: train loss: 0.04908782243728638\n",
      "Epoch 985: train loss: 0.04908609762787819\n",
      "Epoch 986: train loss: 0.0490843802690506\n",
      "Epoch 987: train loss: 0.04908265918493271\n",
      "Epoch 988: train loss: 0.049080926924943924\n",
      "Epoch 989: train loss: 0.049079228192567825\n",
      "Epoch 990: train loss: 0.04907749965786934\n",
      "Epoch 991: train loss: 0.04907579347491264\n",
      "Epoch 992: train loss: 0.04907408356666565\n",
      "Epoch 993: train loss: 0.04907238110899925\n",
      "Epoch 994: train loss: 0.04907066002488136\n",
      "Epoch 995: train loss: 0.049068957567214966\n",
      "Epoch 996: train loss: 0.04906727373600006\n",
      "Epoch 997: train loss: 0.04906556382775307\n",
      "Epoch 998: train loss: 0.049063872545957565\n",
      "Epoch 999: train loss: 0.04906218498945236\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class NN(torch.nn.Module):\n",
    "    def __init__(self,n_input=4, n_output=1):\n",
    "        super(NN, self).__init__()\n",
    "        self.fc = torch.nn.Linear(n_input,n_output)\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "net = NN(n_input=4, n_output=1)\n",
    "\n",
    "model = NN(4, 1)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\n",
    "epoch=1000\n",
    "\n",
    "X = data['data']\n",
    "Y = data['target']\n",
    "x_train = Variable(torch.tensor(X,dtype=torch.float))\n",
    "y_train = Variable(torch.tensor(Y,dtype=torch.float))\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(epoch):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward\n",
    "    y_pred = model(x_train)\n",
    "\n",
    "    # Loss\n",
    "    loss = criterion(y_pred.squeeze(), y_train)\n",
    "    loss_history.append(loss)\n",
    "\n",
    "    print(f'Epoch {epoch}: train loss: {loss.item()}')\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x7f50719404f0>]"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_history)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}